# Ravena_R1

## 🚩 项目简介

Ravena_R1 是一个基于 Transformer 架构的中文对话生成模型，结合了 **强化学习 (RL)**、**推理链 (CoT)**、**元学习 (ML)**、**马尔可夫决策过程 (MDP)**、**强化学习与人类反馈 (RLHF)** 和 **大规模语言模型 (LLM)** 的先进技术，能够根据用户输入的中文问题生成流畅、相关的回答。该项目基于 TensorFlow 和 Keras 实现，利用 Jieba 分词和创新的数据增强方法，致力于提供高效、可定制的对话系统解决方案。

## ❔ 主要特点

- **多层 Transformer 架构**：采用了 120 层多头自注意力机制（Multi-Head Attention）和层归一化（Layer Normalization），显著提升模型在复杂上下文中的理解和生成能力。
  
- **自定义位置编码**：通过自定义的 Positional Encoding 层，对输入文本进行位置编码，保留序列中词语的位置信息，使模型能够捕捉文本中词语之间的顺序关系。

- **数据增强**：实现了多种数据增强技术，包括对训练数据中词语顺序的随机打乱以及随机插入/删除词语。这些增强方法有助于提升模型的泛化能力和鲁棒性。

- **中文分词支持**：使用 Jieba 分词工具对中文文本进行精确分词，确保模型能够正确理解中文句子的语法结构和语义。

- **动态学习率调整**：引入了自定义的学习率调度器，包括 Warm-up 和指数衰减策略，以提高训练的稳定性和收敛速度。

- **提前停止与模型检查点**：使用 EarlyStopping 和 ModelCheckpoint 回调函数，自动在验证集上监控训练过程，并保存性能最优的模型，避免过拟合。

- **GPU 支持**：自动检测可用的 GPU 资源，并启用 TensorFlow 的动态显存分配，支持在 GPU 上加速训练和推理过程。

## 🖥️ 功能特点

- **模型训练与优化**：提供训练过程中动态调整学习率的功能，并自动保存最佳模型，防止过拟合，优化模型性能。

- **对话生成**：通过输入文本，模型能够生成上下文相关、自然流畅的回答。支持使用温度（Temperature）和 Top-p 采样策略来调控生成文本的多样性。

- **Tokenizer 保存与加载**：支持将训练好的 Tokenizer 保存为 JSON 文件，方便在不同环境下加载和重用，提高模型的移植性。

- **文本清理与格式化**：在生成回答时，自动进行文本清理和格式化，包括去除多余的空格和标点符号，保证输出文本的质量。

- **数据加载与增强**：从 JSON 格式的文件中加载训练数据，支持对数据进行分词、增强处理以及填充（Padding），提高模型的训练效果。

- **GPU 加速**：通过 TensorFlow 配置自动检测 GPU 资源，启用动态显存分配，减少训练过程中的显存占用，提升训练速度。

- **自定义近义词替换**：支持对常见的词汇进行近义词替换，增加生成文本的多样性和自然性，避免生成重复且机械化的回答。

## 🔍 技术与方法

### 1. 强化学习 (RL) - **Reinforcement Learning**

在 Ravena_R1 中，**强化学习 (RL)** 主要用于提升模型在对话生成中的决策能力。通过交互式训练，模型通过与环境的反馈不断调整生成策略，使得模型在复杂的对话场景中能够做出更优的决策。训练过程中，模型根据上下文和用户输入调整生成答案，强化学习算法帮助模型优化决策过程，从而提高生成的自然语言回答的质量。

### 2. 推理链 (CoT) - **Chain of Thought**

**推理链 (CoT)** 是一种通过逐步推理生成答案的方法。在这个项目中，CoT 用于增强模型的推理能力。通过在模型生成回答时引入多步推理的机制，模型能够更加准确地理解输入问题的复杂性，并生成结构化且逻辑连贯的回答。CoT 使模型能够像人类一样进行推理，从而提高回答的准确性和深度。

### 3. 元学习 (ML) - **Meta-Learning**

**元学习 (ML)** 使得模型能够在多个任务上快速适应，提高其泛化能力。在 Ravena_R1 中，元学习被用于训练过程中让模型学习如何在面对不同种类的对话时进行调整。通过利用先前的任务经验，模型能够更高效地处理新问题，减少过拟合，并且在面对新的、未见过的输入时，能够迅速做出反应。

### 4. 马尔可夫决策过程 (MDP) - **Markov Decision Process**

**马尔可夫决策过程 (MDP)** 是强化学习的核心框架之一。在 Ravena_R1 中，我们通过 MDP 模型对话系统的状态空间进行了定义。系统的状态可以是当前对话中的上下文，而动作则是生成的回答。通过 MDP，模型能够在每个状态下选择最优的行动策略，即选择一个最合适的回答，从而最大化奖励（生成的回答的相关性和流畅度）。

### 5. 强化学习与人类反馈 (RLHF) - **Reinforcement Learning with Human Feedback**

**强化学习与人类反馈 (RLHF)** 通过结合人类专家的反馈来优化强化学习过程。在 Ravena_R1 中，模型能够通过人类反馈来校正其生成的回答。每当模型生成的回答不符合预期时，人类可以提供反馈，帮助模型了解哪些是优质的回答，哪些是低质量的。通过这种反馈机制，模型能够不断改进其生成策略，从而提升模型的整体表现。

### 6. 大规模语言模型 (LLM) - **Large Language Model**

**大规模语言模型 (LLM)** 是 Ravena_R1 的核心技术之一。该模型通过对海量中文文本进行预训练，学会了语言的基本规则和模式，从而能够处理复杂的自然语言任务。LLM 使得模型具备强大的语言理解和生成能力，能够生成符合上下文的高质量回答。通过大规模数据的训练，模型不仅学会了词汇的使用，还能够理解文本的深层次语义和逻辑关系。

## 🏗️ 架构设计

1. **数据处理模块**：该模块负责加载训练数据（JSON 格式），并对数据进行分词、增强、填充等预处理操作。使用 Jieba 对中文文本进行分词，以保证处理中文时的准确性。

2. **模型架构**：Ravena_R1 基于 Transformer 架构，由多个自注意力层（Multi-Head Attention）和层归一化（Layer Normalization）构成。采用 120 层 Transformer 模型，通过位置编码（Positional Encoding）保留输入文本的序列信息，并使用残差连接提升模型的训练稳定性。

3. **训练与优化**：使用 TensorFlow 和 Keras 提供的 Adam 优化器进行训练，支持动态调整学习率。通过早停和模型检查点回调函数，在验证集上监控模型训练进度，自动保存最佳模型。

4. **生成模块**：通过输入问题，模型能够生成相关的答案。生成过程中使用了温度调控和 Top-p 采样策略，确保生成文本的多样性和流畅性。同时，支持自定义的近义词替换功能，增加生成文本的自然性。

5. **GPU 加速**：自动检测系统是否支持 GPU，并启用 TensorFlow 的动态显存分配，确保在有 GPU 环境下高效训练和推理。

## 🖥️功能特点

- **模型训练与优化**：支持训练过程中动态调整学习率，自动保存最佳模型并避免过拟合。
- **对话生成**：通过输入文本，模型能够生成上下文相关的自然语言回答。支持使用温度和 top-p 采样方法来控制生成文本的多样性。
- **Tokenizer 保存与加载**：支持保存和加载 Tokenizer，使得模型可以在不同环境下共享或重用训练数据。
- **文本清理与格式化**：在生成回答时，自动进行文本清理和格式化，保证输出文本的质量。
- **数据加载与增强**：从 JSON 格式的文件中加载训练数据，并对数据进行分词和增强处理，提升模型训练效果。
- **GPU 加速**：通过 `tf.config` 检测 GPU 是否可用，并启用动态显存分配，减少训练过程中的显存占用，提升训练速度。
- **自定义近义词替换**：在生成文本时，支持对常见的词汇进行近义词替换，提升回答的多样性和自然性。


## ⚠️版权声明

- **非商业性使用**：作品可以被共享、复制和修改，但仅限于非商业用途。
- **禁止商业用途**：作品不得用于商业目的，如销售、租赁或任何形式的商业经营。
- **署名要求**：在使用作品时，必须适当地标明作者并提供协议副本。
- **免责声明**：作品是“按原样”提供的，作者不承担因使用作品而产生的任何责任。

## 🫥环境要求

### 📦必要依赖
#### Linux系统即可
- tensorflow
- flask
- pydot
- graphviz
- jieba
- numpy
- termcolor
- tqdm
- keras
- h5py
- flask-cors
- requests
- scikit-learn

### 🥽安装依赖

您可以通过以下命令安装所有依赖项：
```bash
pip install -r requirements.txt
```
## 🧾 数据格式

项目支持自定义数据集，数据需以 JSON 格式存储，每个数据项包含问题（`question`）和回答（`answer`）。示例格式如下：

```json
{
  "data": [
    {
      "question": "你是谁？",
      "answer": "我是一个语言模型。"
    },
    {
      "question": "今天的天气如何？",
      "answer": "今天天气晴，适合出门。"
    }
  ]
}
```

## 🗝️ 修改数据集
打开 train_data.json 文件。
添加新的问题和答案，确保每对数据包含 question 和 answer 字段。
保存文件并重新运行训练脚本。
根据需要，您可以扩展数据集，添加更多的问答对，适应不同领域的需求。
这个版本更简洁明了，直接说明了如何修改数据集和保存文件。

## 😅如何训练模型
准备数据：修改 train_data.json 文件，确保数据格式正确。
训练模型：运行 train.py 文件来开始训练，默认情况下，模型会进行 500 轮训练。如果您希望修改训练轮数，可以调整 model.py 中的 ```patience=500```
交互使用：训练完成后，程序会进入一个交互模式，您可以输入问题并接收模型生成的回答。输入 exit 可退出程序。生成回答当模型训练完成后，您可以通过交互模式提问，模型会返回生成的回答。生成的回答将被逐步显示，每个部分都会用 "回答1", "回答2" 等标号表示，直到回答完整为止。


## 🧑‍🏫 如何微调模型

微调模型的步骤如下：
微调（Fine-Tuning）是指在已有模型的基础上，使用新的数据集或更小的学习率进行训练。对于这个项目，微调模型可以通过以下几种方式进行：

### 1. 准备微调数据集：
微调模型需要您提供一个新的数据集，这个数据集应包含新的问答对，并且格式必须与原始训练数据格式相同（question 和 answer 字段）。请参考上面提供的数据格式来准备您的数据集。

### 2. 加载已有模型：
在微调之前，我们需要加载已经训练好的模型。在 `train.py` 中，您可以指定已有模型文件进行加载。例如，如果您已经训练过模型并且保存了 `Ravena-LLM_Model.keras` 文件，可以直接加载该模型。

```python
model = LanguageModel(model_file='model/Ravena-LLM_Model.keras')
```
### 3. 微调参数设置：
微调时，您可以调整一些参数，特别是 `epochs` 和 `learning_rate`。通常，微调时我们会使用较小的学习率，例如 0.0001，这样可以避免模型发生剧烈的更新。您可以在 `train.py` 中进行调整。

```python
model.train(epochs=20, batch_size=32)  # 例如将训练轮数设置为 20
```
### 4. 训练微调模型：
运行 `train.py` 文件开始微调。微调过程中，模型会根据新的数据集继续训练，但不会像初始训练那样完全改变模型结构，而是根据新的数据进行适应性学习。

### 5. 保存微调后的模型：
微调完成后，您可以将新的模型保存到文件中，便于之后使用。
```python
model.model.save('model/Ravena-LLM_Model_Finetuned.keras')
```
### 6. 微调后的效果：
微调的目的是让模型更好地适应特定领域的数据，而不是从零开始训练。因此，微调通常比从头训练节省时间，并且能取得更好的效果。

在模型微调完成后，您可以继续通过 `run.py`进行交互，测试微调后的模型是否能够更好地回答问题。
## 🫠项目结构
```bash
├── model.py              # 模型定义文件，包含 Transformer 网络和训练代码
├── run.py                # 运行脚本
├── LICENSE               # 许可证
├── model_accuracy_20241115_214946_ea6085ad.png  #模型质量图
├── train.py              # 训练脚本
├── train_data.json       # 训练数据文件（包含 question 和 answer）
├── test.py               # 生成图片文件主要用于,查看模型性能
├── api.py                # 模型API
├── testapi.py            # 测试API
├── fine_tune.py          # 微调脚本
├── Ravena-WebUI.html     # 模型前端界面要求运行API.py
├── requirements.txt      # 项目依赖文件
├── Ravena_tokenizer_output.txt  # 序列Tokenizer对象可查看的版本通过test.py生成
├── Tokenizer.py          # 生成可查看的 Tokenizer 
├── /visualize model
  ├── model_structure.png # 生成后模型框架图
  └── visualize_model.py  #生成脚本
├── /weights
  ├── model_weights.png   # 生成后权重图
  ├── 生成权重.py          # 生成权重可视化文件
  └── model_weights.txt   #权重可视化文本
├── /img-model
  ├── image_model_inference.py # 模型推理
  ├── image_model.py          # 模型定义文件
├──tts
  ├── tts_run.py             # 运行脚本
  └── tts_train.py          # 训练脚本
├── /model
  ├── Ravena-LLM_Model.keras # 训练好的模型权重文件
  └── tokenizer.json         # 用于将文本转化为数字序列的 Tokenizer 对象
└── README.md                # 项目目录以及内容相同，支持英文对话，参数量更大。
```

## 贡献
### 如果您有改进建议或想要贡献代码，欢迎通过 GitHub 提交 issues 或 pull requests。
### 通过 GitHub Issues 提交问题
### 通过 Email:anan15919991635@163.com 联系我


## 😁相关论文和文章
#### 《Sequence to Sequence Learning with Neural Networks》 - Google 深度学习团队提出的 Seq2Seq 模型。
#### 《Long Short-Term Memory》 - LSTM 网络的原始论文，介绍了 LSTM 如何解决传统 RNN 的长程依赖问题。
#### 《A Survey on Language Models》 - 综述文章，介绍了现代语言模型的进展，包括基于 Transformer 的模型（如 GPT 和 BERT）等。